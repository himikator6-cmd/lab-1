import pickle
from L1_Modules import text_cleaner as tc
from L1_Modules import subword_tokenizers as st


TestText1 = """События игр серии показываются с точки неназванного космического пехотинца,
работающего на Объединённую Аэрокосмическую Корпорацию (англ. Union Aerospace Corporation, UAC)
и сражающегося против полчищ демонов, для того чтобы выжить и спасти Землю от их нападения.
Действие почти всех частей серии происходит на Марсе, его спутниках, Фобосе и Деймосе, в лабораториях ОАК или в Аду."""

TestText2 = """Оригинальная игра 1993 года является одной из первых игр от первого лица для IBM-PC-совместимых компьютеров
с псевдотрёхмерной графикой,многопользовательским режимом, а также с поддержкой пользовательских модификаций.
К большей части игр серии были выпущены многочисленные модификации.
Сопутствующая продукция представлена комиксом, двумя одноимёнными фильмами и несколькими книгами по мотивам сюжета."""


print()
BPE = None
with open("Encoders/BPE_Tokenizer_Encoder_8000_2.pkl", 'rb') as f:
    BPE = pickle.load(f)
BPE1 = st.Tokenizer(TestText1, BPE)
BPE2 = st.Tokenizer(TestText2, BPE)
print("BPE Токенизация 8000 2")
print()
print()
print(BPE1[0])
print()
print()
print(BPE2[0])
print()
print()
print()
print()
BPE = None
with open("Encoders/BPE_Tokenizer_Encoder_8000_3.pkl", 'rb') as f:
    BPE = pickle.load(f)
BPE1 = st.Tokenizer(TestText1, BPE)
BPE2 = st.Tokenizer(TestText2, BPE)
print("BPE Токенизация 8000 3")
print()
print()
print(BPE1[0])
print()
print()
print(BPE2[0])
print()
print()
print()
print()
BPE = None
with open("Encoders/BPE_Tokenizer_Encoder_8000_4.pkl", 'rb') as f:
    BPE = pickle.load(f)
BPE1 = st.Tokenizer(TestText1, BPE)
BPE2 = st.Tokenizer(TestText2, BPE)
print("BPE Токенизация 8000 4")
print()
print()
print(BPE1[0])
print()
print()
print(BPE2[0])
print()
print()
print()
print()
BPE = None
with open("Encoders/BPE_Tokenizer_Encoder_8000_5.pkl", 'rb') as f:
    BPE = pickle.load(f)
BPE1 = st.Tokenizer(TestText1, BPE)
BPE2 = st.Tokenizer(TestText2, BPE)
print("BPE Токенизация 8000 5")
print()
print()
print(BPE1[0])
print()
print()
print(BPE2[0])
print()
print()
print()
print()
BPE = None
with open("Encoders/BPE_Tokenizer_Encoder_16000_2.pkl", 'rb') as f:
    BPE = pickle.load(f)
BPE1 = st.Tokenizer(TestText1, BPE)
BPE2 = st.Tokenizer(TestText2, BPE)
print("BPE Токенизация 16000 2")
print()
print()
print(BPE1[0])
print()
print()
print(BPE2[0])
print()
print()
print()
print()
BPE = None
with open("Encoders/BPE_Tokenizer_Encoder_16000_3.pkl", 'rb') as f:
    BPE = pickle.load(f)
BPE1 = st.Tokenizer(TestText1, BPE)
BPE2 = st.Tokenizer(TestText2, BPE)
print("BPE Токенизация 16000 3")
print()
print()
print(BPE1[0])
print()
print()
print(BPE2[0])
print()
print()
print()
print()
BPE = None
with open("Encoders/BPE_Tokenizer_Encoder_16000_4.pkl", 'rb') as f:
    BPE = pickle.load(f)
BPE1 = st.Tokenizer(TestText1, BPE)
BPE2 = st.Tokenizer(TestText2, BPE)
print("BPE Токенизация 16000 4")
print()
print()
print(BPE1[0])
print()
print()
print(BPE2[0])
print()
print()
print()
print()
BPE = None
with open("Encoders/BPE_Tokenizer_Encoder_16000_5.pkl", 'rb') as f:
    BPE = pickle.load(f)
BPE1 = st.Tokenizer(TestText1, BPE)
BPE2 = st.Tokenizer(TestText2, BPE)
print("BPE Токенизация 16000 5")
print()
print()
print(BPE1[0])
print()
print()
print(BPE2[0])
print()
print()
print()
print()
BPE = None
with open("Encoders/BPE_Tokenizer_Encoder_24000_2.pkl", 'rb') as f:
    BPE = pickle.load(f)
BPE1 = st.Tokenizer(TestText1, BPE)
BPE2 = st.Tokenizer(TestText2, BPE)
print("BPE Токенизация 24000 2")
print()
print()
print(BPE1[0])
print()
print()
print(BPE2[0])
print()
print()
print()
print()
BPE = None
with open("Encoders/BPE_Tokenizer_Encoder_24000_3.pkl", 'rb') as f:
    BPE = pickle.load(f)
BPE1 = st.Tokenizer(TestText1, BPE)
BPE2 = st.Tokenizer(TestText2, BPE)
print("BPE Токенизация 24000 3")
print()
print()
print(BPE1[0])
print()
print()
print(BPE2[0])
print()
print()
print()
print()
BPE = None
with open("Encoders/BPE_Tokenizer_Encoder_24000_4.pkl", 'rb') as f:
    BPE = pickle.load(f)
BPE1 = st.Tokenizer(TestText1, BPE)
BPE2 = st.Tokenizer(TestText2, BPE)
print("BPE Токенизация 24000 4")
print()
print()
print(BPE1[0])
print()
print()
print(BPE2[0])
print()
print()
print()
print()
BPE = None
with open("Encoders/BPE_Tokenizer_Encoder_24000_5.pkl", 'rb') as f:
    BPE = pickle.load(f)
BPE1 = st.Tokenizer(TestText1, BPE)
BPE2 = st.Tokenizer(TestText2, BPE)
print("BPE Токенизация 24000 5")
print()
print()
print(BPE1[0])
print()
print()
print(BPE2[0])
print()
print()
print()
print()
BPE = None
with open("Encoders/BPE_Tokenizer_Encoder_32000_2.pkl", 'rb') as f:
    BPE = pickle.load(f)
BPE1 = st.Tokenizer(TestText1, BPE)
BPE2 = st.Tokenizer(TestText2, BPE)
print("BPE Токенизация 32000 2")
print()
print()
print(BPE1[0])
print()
print()
print(BPE2[0])
print()
print()
print()
print()
BPE = None
with open("Encoders/BPE_Tokenizer_Encoder_32000_3.pkl", 'rb') as f:
    BPE = pickle.load(f)
BPE1 = st.Tokenizer(TestText1, BPE)
BPE2 = st.Tokenizer(TestText2, BPE)
print("BPE Токенизация 32000 3")
print()
print()
print(BPE1[0])
print()
print()
print(BPE2[0])
print()
print()
print()
print()
BPE = None
with open("Encoders/BPE_Tokenizer_Encoder_32000_4.pkl", 'rb') as f:
    BPE = pickle.load(f)
BPE1 = st.Tokenizer(TestText1, BPE)
BPE2 = st.Tokenizer(TestText2, BPE)
print("BPE Токенизация 32000 4")
print()
print()
print(BPE1[0])
print()
print()
print(BPE2[0])
print()
print()
print()
print()
BPE = None
with open("Encoders/BPE_Tokenizer_Encoder_32000_5.pkl", 'rb') as f:
    BPE = pickle.load(f)
BPE1 = st.Tokenizer(TestText1, BPE)
BPE2 = st.Tokenizer(TestText2, BPE)
print("BPE Токенизация 32000 5")
print()
print()
print(BPE1[0])
print()
print()
print(BPE2[0])
print()
print()
print()
print()
WP = None
with open("Encoders/WordPiece_Tokenizer_Encoder_8000_2.pkl", 'rb') as f:
    WP = pickle.load(f)
WP1 = st.Tokenizer(TestText1, WP)
WP2 = st.Tokenizer(TestText2, WP)
print("WordPiece Токенизация 8000 2")
print()
print()
print(WP1[0])
print()
print()
print(WP2[0])
print()
print()
print()
print()
WP = None
with open("Encoders/WordPiece_Tokenizer_Encoder_8000_3.pkl", 'rb') as f:
    WP = pickle.load(f)
WP1 = st.Tokenizer(TestText1, WP)
WP2 = st.Tokenizer(TestText2, WP)
print("WordPiece Токенизация 8000 3")
print()
print()
print(WP1[0])
print()
print()
print(WP2[0])
print()
print()
print()
print()
WP = None
with open("Encoders/WordPiece_Tokenizer_Encoder_8000_4.pkl", 'rb') as f:
    WP = pickle.load(f)
WP1 = st.Tokenizer(TestText1, WP)
WP2 = st.Tokenizer(TestText2, WP)
print("WordPiece Токенизация 8000 4")
print()
print()
print(WP1[0])
print()
print()
print(WP2[0])
print()
print()
print()
print()
WP = None
with open("Encoders/WordPiece_Tokenizer_Encoder_8000_5.pkl", 'rb') as f:
    WP = pickle.load(f)
WP1 = st.Tokenizer(TestText1, WP)
WP2 = st.Tokenizer(TestText2, WP)
print("WordPiece Токенизация 8000 5")
print()
print()
print(WP1[0])
print()
print()
print(WP2[0])
print()
print()
print()
print()
WP = None
with open("Encoders/WordPiece_Tokenizer_Encoder_16000_2.pkl", 'rb') as f:
    WP = pickle.load(f)
WP1 = st.Tokenizer(TestText1, WP)
WP2 = st.Tokenizer(TestText2, WP)
print("WordPiece Токенизация 16000 2")
print()
print()
print(WP1[0])
print()
print()
print(WP2[0])
print()
print()
print()
print()
WP = None
with open("Encoders/WordPiece_Tokenizer_Encoder_16000_3.pkl", 'rb') as f:
    WP = pickle.load(f)
WP1 = st.Tokenizer(TestText1, WP)
WP2 = st.Tokenizer(TestText2, WP)
print("WordPiece Токенизация 16000 3")
print()
print()
print(WP1[0])
print()
print()
print(WP2[0])
print()
print()
print()
print()
WP = None
with open("Encoders/WordPiece_Tokenizer_Encoder_16000_4.pkl", 'rb') as f:
    WP = pickle.load(f)
WP1 = st.Tokenizer(TestText1, WP)
WP2 = st.Tokenizer(TestText2, WP)
print("WordPiece Токенизация 16000 4")
print()
print()
print(WP1[0])
print()
print()
print(WP2[0])
print()
print()
print()
print()
WP = None
with open("Encoders/WordPiece_Tokenizer_Encoder_16000_5.pkl", 'rb') as f:
    WP = pickle.load(f)
WP1 = st.Tokenizer(TestText1, WP)
WP2 = st.Tokenizer(TestText2, WP)
print("WordPiece Токенизация 16000 5")
print()
print()
print(WP1[0])
print()
print()
print(WP2[0])
print()
print()
print()
print()
WP = None
with open("Encoders/WordPiece_Tokenizer_Encoder_24000_2.pkl", 'rb') as f:
    WP = pickle.load(f)
WP1 = st.Tokenizer(TestText1, WP)
WP2 = st.Tokenizer(TestText2, WP)
print("WordPiece Токенизация 24000 2")
print()
print()
print(WP1[0])
print()
print()
print(WP2[0])
print()
print()
print()
print()
WP = None
with open("Encoders/WordPiece_Tokenizer_Encoder_24000_3.pkl", 'rb') as f:
    WP = pickle.load(f)
WP1 = st.Tokenizer(TestText1, WP)
WP2 = st.Tokenizer(TestText2, WP)
print("WordPiece Токенизация 24000 3")
print()
print()
print(WP1[0])
print()
print()
print(WP2[0])
print()
print()
print()
print()
WP = None
with open("Encoders/WordPiece_Tokenizer_Encoder_24000_4.pkl", 'rb') as f:
    WP = pickle.load(f)
WP1 = st.Tokenizer(TestText1, WP)
WP2 = st.Tokenizer(TestText2, WP)
print("WordPiece Токенизация 24000 4")
print()
print()
print(WP1[0])
print()
print()
print(WP2[0])
print()
print()
print()
print()
WP = None
with open("Encoders/WordPiece_Tokenizer_Encoder_24000_5.pkl", 'rb') as f:
    WP = pickle.load(f)
WP1 = st.Tokenizer(TestText1, WP)
WP2 = st.Tokenizer(TestText2, WP)
print("WordPiece Токенизация 24000 5")
print()
print()
print(WP1[0])
print()
print()
print(WP2[0])
print()
print()
print()
print()
WP = None
with open("Encoders/WordPiece_Tokenizer_Encoder_32000_2.pkl", 'rb') as f:
    WP = pickle.load(f)
WP1 = st.Tokenizer(TestText1, WP)
WP2 = st.Tokenizer(TestText2, WP)
print("WordPiece Токенизация 32000 2")
print()
print()
print(WP1[0])
print()
print()
print(WP2[0])
print()
print()
print()
print()
WP = None
with open("Encoders/WordPiece_Tokenizer_Encoder_32000_3.pkl", 'rb') as f:
    WP = pickle.load(f)
WP1 = st.Tokenizer(TestText1, WP)
WP2 = st.Tokenizer(TestText2, WP)
print("WordPiece Токенизация 32000 3")
print()
print()
print(WP1[0])
print()
print()
print(WP2[0])
print()
print()
print()
print()
WP = None
with open("Encoders/WordPiece_Tokenizer_Encoder_32000_4.pkl", 'rb') as f:
    WP = pickle.load(f)
WP1 = st.Tokenizer(TestText1, WP)
WP2 = st.Tokenizer(TestText2, WP)
print("WordPiece Токенизация 32000 4")
print()
print()
print(WP1[0])
print()
print()
print(WP2[0])
print()
print()
print()
print()
WP = None
with open("Encoders/WordPiece_Tokenizer_Encoder_32000_5.pkl", 'rb') as f:
    WP = pickle.load(f)
WP1 = st.Tokenizer(TestText1, WP)
WP2 = st.Tokenizer(TestText2, WP)
print("WordPiece Токенизация 32000 5")
print()
print()
print(WP1[0])
print()
print()
print(WP2[0])
print()
print()
print()
print()
